2025-12-21 16:07:40,518 INFO Starting report generation
2025-12-21 16:07:40,518 WARNING ANTHROPIC_API_KEY not set in environment. Export it before starting the server to enable Anthropic API calls.
2025-12-21 16:07:40,518 WARNING Could not import vectordb_storage.documents_text: No module named 'vectordb_storage'
2025-12-21 16:07:40,666 INFO Loaded docs_meta.json with 22 entries
2025-12-21 16:07:40,670 INFO Loaded FAISS index
2025-12-21 16:07:45,979 INFO LLM_PROVIDER=anthropic; skipping local transformers pipeline initialization
2025-12-21 16:07:45,980 INFO Processing question 1
2025-12-21 16:07:50,077 INFO No LLM provider available; returning placeholder
2025-12-21 16:07:50,078 INFO Processing question 2
2025-12-21 16:07:51,662 INFO Starting report generation
2025-12-21 16:07:51,663 WARNING ANTHROPIC_API_KEY not set in environment. Export it before starting the server to enable Anthropic API calls.
2025-12-21 16:07:51,663 WARNING Could not import vectordb_storage.documents_text: No module named 'vectordb_storage'
2025-12-21 16:07:51,778 INFO Loaded docs_meta.json with 22 entries
2025-12-21 16:07:51,779 INFO Loaded FAISS index
2025-12-21 16:07:56,559 INFO LLM_PROVIDER=anthropic; skipping local transformers pipeline initialization
2025-12-21 16:07:56,562 INFO Processing question 1
2025-12-21 16:07:59,901 INFO No LLM provider available; returning placeholder
2025-12-21 16:07:59,901 INFO Processing question 2
2025-12-21 16:08:03,120 INFO No LLM provider available; returning placeholder
2025-12-21 16:08:03,120 INFO Processing question 3
2025-12-21 16:08:06,303 INFO No LLM provider available; returning placeholder
2025-12-21 16:08:06,303 INFO Processing question 4
2025-12-21 16:08:09,617 INFO No LLM provider available; returning placeholder
2025-12-21 16:08:09,618 INFO Processing question 5
2025-12-21 16:08:13,051 INFO No LLM provider available; returning placeholder
2025-12-21 16:08:13,051 INFO Processing question 6
2025-12-21 16:08:16,289 INFO No LLM provider available; returning placeholder
2025-12-21 16:08:16,290 INFO Processing question 7
2025-12-21 16:08:20,365 INFO No LLM provider available; returning placeholder
2025-12-21 16:08:20,366 INFO Processing question 8
2025-12-21 16:08:23,527 INFO No LLM provider available; returning placeholder
2025-12-21 16:08:23,528 INFO Processing question 9
2025-12-21 16:08:27,608 INFO No LLM provider available; returning placeholder
2025-12-21 16:08:27,608 INFO Processing question 10
2025-12-21 16:08:31,256 INFO No LLM provider available; returning placeholder
2025-12-21 16:08:31,257 INFO Processing question 11
2025-12-21 16:08:34,619 INFO No LLM provider available; returning placeholder
2025-12-21 16:08:34,619 INFO Processing question 12
2025-12-21 16:08:37,908 INFO No LLM provider available; returning placeholder
2025-12-21 16:08:37,908 INFO Processing question 13
2025-12-21 16:08:41,641 INFO No LLM provider available; returning placeholder
2025-12-21 16:08:41,642 INFO Processing question 14
2025-12-21 16:08:44,737 INFO No LLM provider available; returning placeholder
2025-12-21 16:08:44,737 INFO Processing question 15
2025-12-21 16:08:48,114 INFO No LLM provider available; returning placeholder
2025-12-21 16:08:48,114 INFO Processing question 16
2025-12-21 16:08:51,545 INFO No LLM provider available; returning placeholder
2025-12-21 16:08:51,547 INFO Wrote analysis to ./llm_analyses/Pipeline run_ End-to-end test-llm-analysis.txt
2025-12-21 16:08:51,731 INFO Wrote PDF analysis to ./llm_analyses/Pipeline run_ End-to-end test-llm-analysis.pdf
2025-12-21 16:09:26,485 INFO Starting report generation
2025-12-21 16:09:26,485 WARNING ANTHROPIC_API_KEY not set in environment. Export it before starting the server to enable Anthropic API calls.
2025-12-21 16:09:26,485 WARNING Could not import vectordb_storage.documents_text: No module named 'vectordb_storage'
2025-12-21 16:09:26,652 INFO Loaded docs_meta.json with 22 entries
2025-12-21 16:09:26,665 INFO Loaded FAISS index
2025-12-21 16:09:32,121 INFO LLM_PROVIDER=anthropic; skipping local transformers pipeline initialization
2025-12-21 16:09:32,122 INFO Processing question 1
2025-12-21 16:09:35,370 INFO No LLM provider available; returning placeholder
2025-12-21 16:09:35,370 INFO Wrote analysis to ./llm_analyses/Pipeline run_ single-question smoke test-llm-analysis.txt
2025-12-21 16:09:35,429 INFO Wrote PDF analysis to ./llm_analyses/Pipeline run_ single-question smoke test-llm-analysis.pdf
2025-12-21 16:10:58,902 INFO Starting report generation
2025-12-21 16:10:58,902 WARNING ANTHROPIC_API_KEY not set in environment. Export it before starting the server to enable Anthropic API calls.
2025-12-21 16:10:59,064 INFO Loaded docs_meta.json with 22 entries
2025-12-21 16:10:59,065 INFO Loaded FAISS index
2025-12-21 16:11:04,056 INFO LLM_PROVIDER=anthropic; skipping local transformers pipeline initialization
2025-12-21 16:11:04,057 INFO Processing question 1
2025-12-21 16:11:07,483 INFO No LLM provider available; returning placeholder
2025-12-21 16:11:07,483 INFO Wrote analysis to ./llm_analyses/Pipeline run_ warning-free smoke test-llm-analysis.txt
2025-12-21 16:11:07,561 INFO Wrote PDF analysis to ./llm_analyses/Pipeline run_ warning-free smoke test-llm-analysis.pdf
2026-01-08 15:10:38,452 INFO Starting report generation
2026-01-08 15:10:38,452 WARNING ANTHROPIC_API_KEY not set in environment. Export it before starting the server to enable Anthropic API calls.
2026-01-08 15:10:38,672 INFO Loaded docs_meta.json with 22 entries
2026-01-08 15:10:38,673 INFO Loaded FAISS index
2026-01-08 15:10:45,441 INFO Initializing pipeline (device cuda? False)
2026-01-08 15:30:32,447 INFO Processing question 1
2026-01-08 15:30:36,771 ERROR ollama provider failed: [Errno 2] No such file or directory: 'ollama'
Traceback (most recent call last):
  File "/Users/nihaarikaagarwal/Downloads/moral-compass/llama_rag.py", line 168, in _call_llm
    p = subprocess.run(cmd, input=prompt, text=True, capture_output=True, timeout=120)
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py", line 554, in run
    with Popen(*popenargs, **kwargs) as process:
         ~~~~~^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py", line 1036, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                        pass_fds, cwd, env,
                        ^^^^^^^^^^^^^^^^^^^
    ...<5 lines>...
                        gid, gids, uid, umask,
                        ^^^^^^^^^^^^^^^^^^^^^^
                        start_new_session, process_group)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py", line 1966, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
FileNotFoundError: [Errno 2] No such file or directory: 'ollama'
2026-01-08 15:30:36,782 INFO No LLM provider available; returning placeholder
2026-01-08 15:32:59,556 INFO Starting report generation
2026-01-08 15:32:59,556 WARNING ANTHROPIC_API_KEY not set in environment. Export it before starting the server to enable Anthropic API calls.
2026-01-08 15:32:59,714 INFO Loaded docs_meta.json with 22 entries
2026-01-08 15:32:59,715 INFO Loaded FAISS index
2026-01-08 15:33:05,196 INFO LLM_PROVIDER=ollama; skipping local transformers pipeline initialization
2026-01-08 15:33:05,197 INFO Processing question 1
2026-01-08 15:33:08,814 ERROR ollama provider failed: [Errno 2] No such file or directory: 'ollama'
Traceback (most recent call last):
  File "/Users/nihaarikaagarwal/Downloads/moral-compass/llama_rag.py", line 168, in _call_llm
    p = subprocess.run(cmd, input=prompt, text=True, capture_output=True, timeout=120)
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py", line 554, in run
    with Popen(*popenargs, **kwargs) as process:
         ~~~~~^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py", line 1036, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                        pass_fds, cwd, env,
                        ^^^^^^^^^^^^^^^^^^^
    ...<5 lines>...
                        gid, gids, uid, umask,
                        ^^^^^^^^^^^^^^^^^^^^^^
                        start_new_session, process_group)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py", line 1966, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
FileNotFoundError: [Errno 2] No such file or directory: 'ollama'
2026-01-08 15:33:08,825 INFO No LLM provider available; returning placeholder
2026-01-08 15:33:08,825 INFO Wrote analysis to ./llm_analyses/Ollama smoke test after patch-llm-analysis.txt
2026-01-08 15:33:09,040 INFO Wrote PDF analysis to ./llm_analyses/Ollama smoke test after patch-llm-analysis.pdf
2026-01-08 22:30:33,484 INFO Starting report generation
2026-01-08 22:30:33,485 INFO Using LLM provider: ollama with model: llama3.2:latest
2026-01-08 22:30:33,486 ERROR Could not initialize retrieval stack (faiss/sentence-transformers)
Traceback (most recent call last):
  File "/Users/nihaarikaagarwal/Downloads/moral-compass/llama_rag.py", line 373, in generate_report
    import faiss
ModuleNotFoundError: No module named 'faiss'
2026-01-08 22:30:33,490 INFO LLM_PROVIDER=ollama; skipping local transformers pipeline initialization
2026-01-08 22:30:33,492 INFO Processing question 1
2026-01-08 22:30:33,493 INFO sentence-transformers unavailable; using simple token-count embeddings
2026-01-08 22:30:33,494 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-08 22:31:02,557 INFO Ollama Python client returned response
2026-01-08 22:31:02,609 INFO LLM (API) provided response for question 1
2026-01-08 22:31:02,632 INFO Processing question 2
2026-01-08 22:31:02,637 INFO sentence-transformers unavailable; using simple token-count embeddings
2026-01-08 22:31:02,711 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-08 22:31:37,105 INFO Ollama Python client returned response
2026-01-08 22:31:37,112 INFO LLM (API) provided response for question 2
2026-01-08 22:31:37,113 INFO Processing question 3
2026-01-08 22:31:37,122 INFO sentence-transformers unavailable; using simple token-count embeddings
2026-01-08 22:31:37,126 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-08 22:31:59,053 INFO Ollama Python client returned response
2026-01-08 22:31:59,066 INFO LLM (API) provided response for question 3
2026-01-08 22:31:59,070 INFO Wrote analysis to ./llm_analyses/Test Study_ Evaluating Ollama Integration-llm-analysis.txt
2026-01-08 22:31:59,093 ERROR Failed to write PDF (reportlab not available or error)
Traceback (most recent call last):
  File "/Users/nihaarikaagarwal/Downloads/moral-compass/llama_rag.py", line 549, in generate_report
    from reportlab.lib.pagesizes import letter
ModuleNotFoundError: No module named 'reportlab'
2026-01-09 16:37:08,678 INFO Starting report generation
2026-01-09 16:37:08,678 INFO Using LLM provider: ollama with model: llama3.2:latest
2026-01-09 16:37:08,679 ERROR Could not initialize retrieval stack (faiss/sentence-transformers)
Traceback (most recent call last):
  File "/Users/nihaarikaagarwal/Downloads/moral-compass/llama_rag.py", line 373, in generate_report
    import faiss
ModuleNotFoundError: No module named 'faiss'
2026-01-09 16:37:08,684 INFO LLM_PROVIDER=ollama; skipping local transformers pipeline initialization
2026-01-09 16:37:08,685 INFO Processing question 1
2026-01-09 16:37:08,685 INFO sentence-transformers unavailable; using simple token-count embeddings
2026-01-09 16:37:09,118 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 16:37:43,347 INFO Ollama Python client returned response
2026-01-09 16:37:43,353 INFO LLM (API) provided response for question 1
2026-01-09 16:37:43,353 INFO Processing question 2
2026-01-09 16:37:43,356 INFO sentence-transformers unavailable; using simple token-count embeddings
2026-01-09 16:37:43,371 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 16:37:58,342 INFO Ollama Python client returned response
2026-01-09 16:37:58,352 INFO LLM (API) provided response for question 2
2026-01-09 16:37:58,352 INFO Processing question 3
2026-01-09 16:37:58,365 INFO sentence-transformers unavailable; using simple token-count embeddings
2026-01-09 16:37:58,385 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 16:38:03,828 INFO Ollama Python client returned response
2026-01-09 16:38:03,833 INFO LLM (API) provided response for question 3
2026-01-09 16:38:03,833 INFO Processing question 4
2026-01-09 16:38:03,835 INFO sentence-transformers unavailable; using simple token-count embeddings
2026-01-09 16:38:03,840 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 16:38:12,565 INFO Ollama Python client returned response
2026-01-09 16:38:12,565 INFO LLM (API) provided response for question 4
2026-01-09 16:38:12,566 INFO Processing question 5
2026-01-09 16:38:12,566 INFO sentence-transformers unavailable; using simple token-count embeddings
2026-01-09 16:38:12,566 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 16:38:19,468 INFO Ollama Python client returned response
2026-01-09 16:38:19,468 INFO LLM (API) provided response for question 5
2026-01-09 16:38:19,468 INFO Processing question 6
2026-01-09 16:38:19,468 INFO sentence-transformers unavailable; using simple token-count embeddings
2026-01-09 16:38:19,468 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 16:38:31,629 INFO Ollama Python client returned response
2026-01-09 16:38:31,631 INFO LLM (API) provided response for question 6
2026-01-09 16:38:31,631 INFO Processing question 7
2026-01-09 16:38:31,632 INFO sentence-transformers unavailable; using simple token-count embeddings
2026-01-09 16:38:31,636 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 16:38:45,444 INFO Ollama Python client returned response
2026-01-09 16:38:45,448 INFO LLM (API) provided response for question 7
2026-01-09 16:38:45,448 INFO Processing question 8
2026-01-09 16:38:45,450 INFO sentence-transformers unavailable; using simple token-count embeddings
2026-01-09 16:38:45,457 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 16:38:54,309 INFO Ollama Python client returned response
2026-01-09 16:38:54,311 INFO LLM (API) provided response for question 8
2026-01-09 16:38:54,313 INFO Wrote analysis to ./llm_analyses/Study on the Impact of AI-Assisted Learning Tools on Student Performance in Undergraduate Computer Science Courses-llm-analysis.txt
2026-01-09 16:38:54,319 ERROR Failed to write PDF (reportlab not available or error)
Traceback (most recent call last):
  File "/Users/nihaarikaagarwal/Downloads/moral-compass/llama_rag.py", line 549, in generate_report
    from reportlab.lib.pagesizes import letter
ModuleNotFoundError: No module named 'reportlab'
2026-01-09 16:46:04,640 INFO Starting report generation
2026-01-09 16:46:04,641 INFO Using LLM provider: ollama with model: llama3.2:latest
2026-01-09 16:46:05,035 INFO Loaded docs_meta.json with 22 entries
2026-01-09 16:46:05,044 INFO Loaded FAISS index
2026-01-09 16:46:15,477 INFO LLM_PROVIDER=ollama; skipping local transformers pipeline initialization
2026-01-09 16:46:15,478 ERROR Failed to write analysis file
Traceback (most recent call last):
  File "/Users/nihaarikaagarwal/Downloads/moral-compass/llama_rag.py", line 477, in generate_report
    with open(output_filepath, 'w', encoding='utf-8') as file:
         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 63] File name too long: './llm_analyses/_    4_1 Inclusion_Exclusion Criteria__    - INCLUDED_ Students aged 18 and above enrolled in CSE 110_    - EXCLUDED_ Minors _under 18__    - INCLUDED_ Pregnant women_    - INCLUDED_ Prisoners currently enrolled before incarceration_    - INCLUDED_ Economically disadvantaged individuals__    4_2 Rationale__    We include pregnant women and prisoners because excluding them would limit_    generalizability of findings about educational technology__    -llm-analysis.txt'
2026-01-09 16:46:56,182 INFO Starting report generation
2026-01-09 16:46:56,182 INFO Using LLM provider: ollama with model: llama3.2:latest
2026-01-09 16:46:56,301 INFO Loaded docs_meta.json with 22 entries
2026-01-09 16:46:56,303 INFO Loaded FAISS index
2026-01-09 16:47:03,207 INFO LLM_PROVIDER=ollama; skipping local transformers pipeline initialization
2026-01-09 16:47:03,213 INFO Processing question 1
2026-01-09 16:47:07,251 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 16:47:58,217 INFO Ollama Python client returned response
2026-01-09 16:47:58,220 INFO LLM (API) provided response for question 1
2026-01-09 16:47:58,221 INFO Processing question 2
2026-01-09 16:48:06,198 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 16:48:52,957 INFO Ollama Python client returned response
2026-01-09 16:48:52,972 INFO LLM (API) provided response for question 2
2026-01-09 16:48:52,975 INFO Processing question 3
2026-01-09 16:49:31,065 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 16:50:05,764 INFO Ollama Python client returned response
2026-01-09 16:50:05,767 INFO LLM (API) provided response for question 3
2026-01-09 16:50:05,769 INFO Processing question 4
2026-01-09 16:50:15,072 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 16:50:48,865 INFO Ollama Python client returned response
2026-01-09 16:50:48,867 INFO LLM (API) provided response for question 4
2026-01-09 16:50:48,869 INFO Wrote analysis to ./llm_analyses/IRB Citation Test - Vulnerable Populations and Consent-llm-analysis.txt
2026-01-09 16:50:48,875 ERROR Failed to write PDF (reportlab not available or error)
Traceback (most recent call last):
  File "/Users/nihaarikaagarwal/Downloads/moral-compass/llama_rag.py", line 579, in generate_report
    from reportlab.lib.pagesizes import letter
ModuleNotFoundError: No module named 'reportlab'
2026-01-09 16:55:23,927 INFO Starting report generation
2026-01-09 16:55:23,927 INFO Using LLM provider: ollama with model: llama3.2:latest
2026-01-09 16:55:24,147 INFO Loaded docs_meta.json with 22 entries
2026-01-09 16:55:24,150 INFO Loaded FAISS index
2026-01-09 16:55:30,748 INFO LLM_PROVIDER=ollama; skipping local transformers pipeline initialization
2026-01-09 16:55:30,749 INFO Processing question 1
2026-01-09 16:55:35,553 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 16:56:09,272 INFO Ollama Python client returned response
2026-01-09 16:56:09,273 INFO LLM (API) provided response for question 1
2026-01-09 16:56:09,274 INFO Processing question 2
2026-01-09 16:56:13,691 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 16:56:45,568 INFO Ollama Python client returned response
2026-01-09 16:56:45,570 INFO LLM (API) provided response for question 2
2026-01-09 16:56:45,570 INFO Processing question 3
2026-01-09 16:56:50,981 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 16:57:21,584 INFO Ollama Python client returned response
2026-01-09 16:57:21,587 INFO LLM (API) provided response for question 3
2026-01-09 16:57:21,587 INFO Processing question 4
2026-01-09 16:57:25,645 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 16:58:00,350 INFO Ollama Python client returned response
2026-01-09 16:58:00,352 INFO LLM (API) provided response for question 4
2026-01-09 16:58:00,353 INFO Processing question 5
2026-01-09 16:58:27,210 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 16:59:11,839 INFO Ollama Python client returned response
2026-01-09 16:59:11,842 INFO LLM (API) provided response for question 5
2026-01-09 16:59:11,842 INFO Processing question 6
2026-01-09 16:59:17,368 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 17:00:05,091 INFO Ollama Python client returned response
2026-01-09 17:00:05,092 INFO LLM (API) provided response for question 6
2026-01-09 17:00:05,093 INFO Processing question 7
2026-01-09 17:00:09,257 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 17:00:47,846 INFO Ollama Python client returned response
2026-01-09 17:00:47,849 INFO LLM (API) provided response for question 7
2026-01-09 17:00:47,849 INFO Processing question 8
2026-01-09 17:00:52,223 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 17:01:38,531 INFO Ollama Python client returned response
2026-01-09 17:01:38,533 INFO LLM (API) provided response for question 8
2026-01-09 17:01:38,534 INFO Processing question 9
2026-01-09 17:01:42,920 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 17:02:21,863 INFO Ollama Python client returned response
2026-01-09 17:02:21,868 INFO LLM (API) provided response for question 9
2026-01-09 17:02:21,869 INFO Processing question 10
2026-01-09 17:02:32,757 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 17:03:11,756 INFO Ollama Python client returned response
2026-01-09 17:03:11,763 INFO LLM (API) provided response for question 10
2026-01-09 17:03:11,768 INFO Processing question 11
2026-01-09 17:03:16,603 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 17:04:06,346 INFO Ollama Python client returned response
2026-01-09 17:04:06,349 INFO LLM (API) provided response for question 11
2026-01-09 17:04:06,350 INFO Processing question 12
2026-01-09 17:04:20,088 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 17:05:27,167 INFO Ollama Python client returned response
2026-01-09 17:05:27,174 INFO LLM (API) provided response for question 12
2026-01-09 17:05:27,179 INFO Processing question 13
2026-01-09 17:05:40,325 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 17:06:18,767 INFO Ollama Python client returned response
2026-01-09 17:06:18,772 INFO LLM (API) provided response for question 13
2026-01-09 17:06:18,774 INFO Wrote analysis to ./llm_analyses/Mental Health Support App Study for College Students-llm-analysis.txt
2026-01-09 17:06:18,784 ERROR Failed to write PDF (reportlab not available or error)
Traceback (most recent call last):
  File "/Users/nihaarikaagarwal/Downloads/moral-compass/llama_rag.py", line 579, in generate_report
    from reportlab.lib.pagesizes import letter
ModuleNotFoundError: No module named 'reportlab'
2026-01-09 18:15:02,631 INFO Starting report generation
2026-01-09 18:15:02,631 INFO Using LLM provider: ollama with model: llama3.2:latest
2026-01-09 18:15:02,745 INFO Loaded docs_meta.json with 22 entries
2026-01-09 18:15:02,747 INFO Loaded FAISS index
2026-01-09 18:15:08,105 INFO LLM_PROVIDER=ollama; skipping local transformers pipeline initialization
2026-01-09 18:15:08,106 INFO Processing question 1
2026-01-09 18:15:11,715 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 18:15:52,297 INFO Ollama Python client returned clean response
2026-01-09 18:15:52,308 INFO LLM (API) provided response for question 1
2026-01-09 18:15:52,308 INFO Processing question 2
2026-01-09 18:15:58,834 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 18:16:29,544 INFO Ollama Python client returned clean response
2026-01-09 18:16:29,547 INFO LLM (API) provided response for question 2
2026-01-09 18:16:29,548 INFO Processing question 3
2026-01-09 18:16:37,382 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 18:17:04,724 INFO Ollama Python client returned clean response
2026-01-09 18:17:04,725 INFO LLM (API) provided response for question 3
2026-01-09 18:17:04,725 INFO Wrote analysis to ./llm_analyses/Formatting Test - Readable IRB Analysis Output-llm-analysis.txt
2026-01-09 18:17:04,729 ERROR Failed to write PDF (reportlab not available or error)
Traceback (most recent call last):
  File "/Users/nihaarikaagarwal/Downloads/moral-compass/llama_rag.py", line 607, in generate_report
    from reportlab.lib.pagesizes import letter
ModuleNotFoundError: No module named 'reportlab'
2026-01-09 18:18:49,603 INFO Starting report generation
2026-01-09 18:18:49,604 INFO Using LLM provider: ollama with model: llama3.2:latest
2026-01-09 18:18:49,900 INFO Loaded docs_meta.json with 22 entries
2026-01-09 18:18:49,904 INFO Loaded FAISS index
2026-01-09 18:19:13,293 INFO LLM_PROVIDER=ollama; skipping local transformers pipeline initialization
2026-01-09 18:19:13,319 INFO Processing question 1
2026-01-09 18:19:26,544 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 18:19:56,075 INFO Ollama Python client returned clean response
2026-01-09 18:19:56,075 INFO LLM (API) provided response for question 1
2026-01-09 18:19:56,075 INFO Processing question 2
2026-01-09 18:19:59,656 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 18:20:34,786 INFO Ollama Python client returned clean response
2026-01-09 18:20:34,789 INFO LLM (API) provided response for question 2
2026-01-09 18:20:34,790 INFO Processing question 3
2026-01-09 18:20:40,130 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 18:21:11,862 INFO Ollama Python client returned clean response
2026-01-09 18:21:11,863 INFO LLM (API) provided response for question 3
2026-01-09 18:21:11,865 INFO Wrote analysis to ./llm_analyses/Formatting Test - Readable IRB Analysis Output-llm-analysis.txt
2026-01-09 18:21:11,870 ERROR Failed to write PDF (reportlab not available or error)
Traceback (most recent call last):
  File "/Users/nihaarikaagarwal/Downloads/moral-compass/llama_rag.py", line 598, in generate_report
    from reportlab.lib.pagesizes import letter
ModuleNotFoundError: No module named 'reportlab'
2026-01-09 18:44:07,656 INFO Starting report generation
2026-01-09 18:44:07,656 INFO Using LLM provider: ollama with model: llama3.2:latest
2026-01-09 18:44:07,800 INFO Loaded docs_meta.json with 22 entries
2026-01-09 18:44:07,804 INFO Loaded FAISS index
2026-01-09 18:44:15,798 INFO LLM_PROVIDER=ollama; skipping local transformers pipeline initialization
2026-01-09 18:44:15,799 INFO Processing question 1
2026-01-09 18:44:20,962 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 18:45:08,397 INFO Ollama Python client returned clean response
2026-01-09 18:45:08,399 INFO LLM (API) provided response for question 1
2026-01-09 18:45:08,399 INFO Processing question 2
2026-01-09 18:45:15,832 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 18:45:50,475 INFO Ollama Python client returned clean response
2026-01-09 18:45:50,478 INFO LLM (API) provided response for question 2
2026-01-09 18:45:50,478 INFO Processing question 3
2026-01-09 18:45:57,051 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 18:46:27,075 INFO Ollama Python client returned clean response
2026-01-09 18:46:27,077 INFO LLM (API) provided response for question 3
2026-01-09 18:46:27,078 INFO Processing question 4
2026-01-09 18:46:30,836 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 18:47:01,421 INFO Ollama Python client returned clean response
2026-01-09 18:47:01,425 INFO LLM (API) provided response for question 4
2026-01-09 18:47:01,428 INFO Processing question 5
2026-01-09 18:47:09,656 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 18:47:49,355 INFO Ollama Python client returned clean response
2026-01-09 18:47:49,382 INFO LLM (API) provided response for question 5
2026-01-09 18:47:49,392 INFO Processing question 6
2026-01-09 18:47:54,464 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-09 18:48:30,151 INFO Ollama Python client returned clean response
2026-01-09 18:48:30,153 INFO LLM (API) provided response for question 6
2026-01-09 18:48:30,159 INFO Wrote analysis to ./llm_analyses/Student Data Privacy Study-llm-analysis.txt
2026-01-09 18:48:30,162 ERROR Failed to write PDF (reportlab not available or error)
Traceback (most recent call last):
  File "/Users/nihaarikaagarwal/Downloads/moral-compass/llama_rag.py", line 598, in generate_report
    from reportlab.lib.pagesizes import letter
ModuleNotFoundError: No module named 'reportlab'
2026-01-12 13:54:48,445 INFO Starting report generation
2026-01-12 13:54:48,445 INFO Using LLM provider: ollama with model: llama3.2:latest
2026-01-12 13:54:48,801 INFO Loaded docs_meta.json with 22 entries
2026-01-12 13:54:48,802 INFO Loaded FAISS index
2026-01-12 13:55:00,245 INFO LLM_PROVIDER=ollama; skipping local transformers pipeline initialization
2026-01-12 13:55:00,247 INFO Processing question 1
2026-01-12 13:55:04,424 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-12 13:55:56,888 INFO Ollama Python client returned clean response
2026-01-12 13:55:56,923 INFO LLM (API) provided response for question 1
2026-01-12 13:55:56,929 INFO Processing question 2
2026-01-12 13:56:46,268 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-12 13:57:35,880 INFO Ollama Python client returned clean response
2026-01-12 13:57:35,902 INFO LLM (API) provided response for question 2
2026-01-12 13:57:35,903 INFO Processing question 3
2026-01-12 14:00:10,915 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-12 14:01:26,530 INFO Ollama Python client returned clean response
2026-01-12 14:01:26,557 INFO LLM (API) provided response for question 3
2026-01-12 14:01:26,574 INFO Processing question 4
2026-01-12 14:02:42,805 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-13 11:36:25,273 INFO Starting report generation
2026-01-13 11:36:25,274 INFO Using LLM provider: ollama with model: llama3.2:latest
2026-01-13 11:36:25,679 INFO Loaded docs_meta.json with 22 entries
2026-01-13 11:36:25,679 INFO Loaded FAISS index
2026-01-13 11:36:35,064 INFO LLM_PROVIDER=ollama; skipping local transformers pipeline initialization
2026-01-13 11:36:35,065 INFO Processing question 1
2026-01-13 11:36:38,983 INFO LLM cache hit
2026-01-13 11:36:38,983 INFO LLM (API) provided response for question 1
2026-01-13 11:36:38,983 INFO Processing question 2
2026-01-13 11:36:42,967 INFO LLM cache hit
2026-01-13 11:36:42,967 INFO LLM (API) provided response for question 2
2026-01-13 11:36:42,967 INFO Processing question 3
2026-01-13 11:36:46,915 INFO LLM cache hit
2026-01-13 11:36:46,915 INFO LLM (API) provided response for question 3
2026-01-13 11:36:46,915 INFO Processing question 4
2026-01-13 11:36:51,117 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-13 11:37:35,818 INFO Ollama Python client returned clean response
2026-01-13 11:37:35,819 INFO LLM (API) provided response for question 4
2026-01-13 11:37:35,819 INFO Processing question 5
2026-01-13 11:37:40,832 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-13 11:38:16,675 INFO Ollama Python client returned clean response
2026-01-13 11:38:16,678 INFO LLM (API) provided response for question 5
2026-01-13 11:38:16,680 INFO Processing question 6
2026-01-13 11:38:22,373 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-13 11:38:52,432 INFO Ollama Python client returned clean response
2026-01-13 11:38:52,434 INFO LLM (API) provided response for question 6
2026-01-13 11:38:52,435 INFO Processing question 7
2026-01-13 11:38:56,753 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-13 11:39:27,347 INFO Ollama Python client returned clean response
2026-01-13 11:39:27,348 INFO LLM (API) provided response for question 7
2026-01-13 11:39:27,348 INFO Processing question 8
2026-01-13 11:39:31,884 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-13 11:40:03,234 INFO Ollama Python client returned clean response
2026-01-13 11:40:03,235 INFO LLM (API) provided response for question 8
2026-01-13 11:40:03,237 INFO Processing question 9
2026-01-13 11:40:07,647 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-13 11:40:51,671 INFO Ollama Python client returned clean response
2026-01-13 11:40:51,673 INFO LLM (API) provided response for question 9
2026-01-13 11:40:51,674 INFO Processing question 10
2026-01-13 11:40:56,665 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-13 11:41:46,422 INFO Ollama Python client returned clean response
2026-01-13 11:41:46,427 INFO LLM (API) provided response for question 10
2026-01-13 11:41:46,428 INFO Processing question 11
2026-01-13 11:41:52,221 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-13 11:42:49,417 INFO Ollama Python client returned clean response
2026-01-13 11:42:49,419 INFO LLM (API) provided response for question 11
2026-01-13 11:42:49,421 INFO Processing question 12
2026-01-13 11:42:54,362 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-13 11:43:43,219 INFO Ollama Python client returned clean response
2026-01-13 11:43:43,220 INFO LLM (API) provided response for question 12
2026-01-13 11:43:43,221 INFO Processing question 13
2026-01-13 11:43:47,680 INFO Using Ollama Python client with model: llama3.2:latest
2026-01-13 11:44:32,518 INFO Ollama Python client returned clean response
2026-01-13 11:44:32,520 INFO LLM (API) provided response for question 13
2026-01-13 11:44:32,521 INFO Wrote analysis to ./llm_analyses/Study on the Impact of AI-Assisted Learning Tools on Student Performance in Undergraduate Computer Science Courses-llm-analysis.txt
2026-01-13 11:44:32,525 ERROR Failed to write PDF (reportlab not available or error)
Traceback (most recent call last):
  File "/Users/nihaarikaagarwal/Downloads/moral-compass/llama_rag.py", line 598, in generate_report
    from reportlab.lib.pagesizes import letter
ModuleNotFoundError: No module named 'reportlab'
